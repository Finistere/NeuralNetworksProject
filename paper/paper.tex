\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{enumitem}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

%\ShortHeadings{Ensemble Methods for Robust Feature Selection}{Meil\u{a} and Jordan}
\firstpageno{1}

\begin{document}

\title{Ensemble Methods for Robust Feature Selection}

\author{\name Benjamin Rapier \email benjamin.rabier@campus.tu-berlin.de \\
       \addr Neural Information Processing Group\\
       Technical University Berlin\\
       10623 Berlin, Germany
       \AND
       \name Alexander Goscinski \email a.goscinski@mail.tu-berlin.de \\
       \addr Neural Information Processing Group\\
       Technical University Berlin\\
       10623 Berlin, Germany}

\editor{No Editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Abstract
\end{abstract}

\begin{keywords}
  Ensemble Methods, Robust, 
\end{keywords}

\section{Introduction}

Feature selection is a preprocessing step used in machine learning application to find a small subset of features in order to build a more accurate, simpler and faster model. Moreover it allows domain experts to gain a better understanding of the data, by focusing on the selected subset. However, in domains such as biomedical applications subsequent analysis are costly. Therefore not only the model performance but also the robustness of the feature selection method is important. Domain experts would prefer a more stable algorithm to have more confidence in the selected features. One approach for more robust results are ensemble methods.

Used in Machine Learning and Statistics, ensemble methods combine multiple algorithms in order to obtain better performance. The bagging method, which consists of using the same algorithm on multiple subsets, has already been studied \cite{saeys2008}. The method improved considerably the robustness of the feature selection algorithm and the model performance was not significantly degraded in most cases. The purpose of this paper is to investigate the effect of ensemble methods further by combining different feature selection methods.

\section{Feature selection methods}

Symmetrical Uncertainty \citep{press1996numerical} is entropy based measurement. It is the normalized mutual information between the feature and the class. The mutual information measures the influence of the knowledge of the class on the feature value in terms of entropy and normalizes it. More details about mutual information can be found in \cite{paninski2003estimation}.
\begin{equation}
  \label{eq:su}
  SU(F,C) = 2 \frac{H(F) - H(F|C)}{H(F) + H(C)} \textrm{, where } H(\cdot) \textrm{ is the entropy.}
\end{equation}

The second feature selection is RELIEFF \citep{kononenko1997overcoming}. For each sample $x_i$ the k nearest neighboring samples, using the $L_1$ norm, belonging the same class $\{\textrm{Near-hit}_{i, j}\}_{j=1..k}$ and the ones of the opposite class $\{\textrm{Near-miss}_{i, j}\}_{j=1..k}$ are selected. Then the difference is taken to update the weight vector representing how well each feature distinguishes samples of the two classes, as can be seen in equation \ref{eq:relief}. In all the experiments k was set to 5, which, empirically, gives satisfactory results. 
\begin{equation}
  \label{eq:relief}
  W_i = W_i - \frac{1}{k}\sum_{j=1}^{k}|x_i - \textrm{Near-hit}_{i,j}| + \frac{1}{k}\sum_{j=1}^{k} |x_i - \textrm{Near-miss}_{i,j}|
\end{equation}

The third feature selection method used is a recursive feature elimination (RFE) with SVM \citep{guyon2002gene}. Recursive feature elimination fits a classifier and ranks the feature according to their importance in the model. In the case of the SVM, it is based on the weight vector of the hyperplane. More precisely after obtaining the solution for the primal problem of the SVM in equation \ref{eq:svm_primal}, the ranking coefficients $c_i$ for the features are then calculated by squaring the weights $c_i = w_i^2$.
\begin{alignat}{-1}
     \min_{w,b}  & \quad \frac12\|w\|^2 + C\sum_{i=1}^m \xi_i & & & \label{eq:svm_primal} \\ 
   \text{s.\,t.} & \quad y^{(i)} (w^Tx^{(i)} + b) & \quad \geq & \quad 1 -\xi_i &&
                   \quad \forall \, i = 1, \ldots, m \nonumber \\
                 & \quad \xi_i                  & \quad \geq & \quad 0 &&
                   \quad \forall \, i = 1, \ldots, m \nonumber 
\end{alignat}

The worst ranked features are then discarded and the procedure is repeated until only a subset of features is left. In the experiments, $10\%$ of the size of all features were discarded at each step. The procedure was repeated until a subset of the size of $1\%$ of all features was left. Since each step $10\%$ of the features are discarded, the overall process takes $10$ steps. Therefore a SVM is $10$ times needed to be evaluated. Another method would be to discard only $10\%$ of the remaining features. A more gradually ascend of feature weights can be achievedthis way. However this was not computable with our ressources. For the hpyerparameter $C$ a grid search on the set $/{1, 10, 100, 1000\}$ was done. The parameters were evaluated with stratisfied 5-fold cross-validation.

The last feature selection method used is Lasso logistc regression \citep{wheeler2010lasso}. It is a logistic regression with a $L_1$ norm as penalty for the weight vector. For the hyperparameter $C$ a grid search with $10$ different parameters in logarithmic scale between $10^{-5}$ and $10^3$ was done. The parameters were evaluated with a stratisfied 3-fold cross-validation.

\begin{equation}
  \min_{w,c} \quad \|w\|_1 + C\sum_{i=1}^n \log(\exp(y_i(x_i^Tw + c))+1)
\end{equation}

The advantage of this method is that we obtain a sparse weight vector as solution. This means only a small subset of features will have a significant high weight.
\section{Ensemble methods}

The different feature selection methods give as output a weight vector representing how well each feature distinguishes samples of the two classes. The ensemble method used aggregates the different weights vectors. Since different methods do not always have the same weight range, they are normalized to the range $[0,1]$. Moreover for the methods which can only rank data, such as methods based on RFE, their ranking are considered as weights, and so subsequently normalized the same way. 

Those normalized weights are combined using different techniques. The methods used in this paper can be grouped in three distinct categories: 

\begin{description}[align=left]
\item [Linear aggregation :] The average of the weights was taken as the new weight.
\item [Non-linear aggregation :] Two methods were used, first averaging powers of the weights and secondly aggregating linearly the minimum, maximum and the average of the weights.
\item [Performance related aggregation :] For each feature selector the best 1\% of the features were selected and used to measure the classification performance for multiple classifiers. The feature selection methods result were weighted accordingly to their accuracy.
\end{description}

\section{Evaluation}

While the purpose of this paper is to find a robust feature selection method, it is also necessary to evaluate the classification performance as domain export have no interest in badly performing model. Hence those two aspects are always considered in combination

\subsection{Data Sets}
Datasets are taken from the NIPS 2003 challenge \cite{NIPS}. However, due to computation power constraints not the whole dataset is always used. Additionally the colon data set \cite{alon1999broad} is used. Finally artificial data is also generated, since it is possible to control which features are meaningful. 

\begin{center}
    \begin{tabular}{| r | c | c | c | c | c |}
    \hline
    Name & \# Samples & \# Features & \# Class 1 & \# Class 2 & Ref\\ \hline
    Arcene & 200 & 10 000 & 112 & 88 & \cite{NIPS} \\
    Dexter & 600 & 20 000 & 300 & 300 & \cite{NIPS} \\
    Gisette & 200 & 5000 & 92 & 108 & \cite{NIPS} \\
    Colon & 62 & 2000 & 40 & 22 &  \cite{alon1999broad} \\
    Artificial & 300 & 10 000 & 150 & 150 & \\
    \hline
    \end{tabular}
\end{center}

The features of the artificial dataset are generated with two distributions, depending on whether they are used in the labeling process or not.

\begin{align}
f \sim0.5 * \left[ N(0, 1) + N(0, \Sigma) \right]  \quad \text{with} \quad \Sigma_{ij} \sim U(0,1)
\end{align}
% \text{unsignificant} \quad & f \sim 0.5 * \left[ N(0, 1) + U(-1,1) \right]

To generate balanced binary labels, the significant features are first summed, then the median of the sums is subtracted, finally the sign is taken. 





\subsection{Experiments}

The robustness is evaluated with a ShuffleSplit of 10 folds with 90\% of the data, as the number of samples is already limited. After weighting the features the best ones are taken (from 0.1\% to 1\%) and those subsets are compared with the Jaccard Index. For two subsets of features $F_i$ and $F_j$ it is defined as:

\begin{equation}
J(F_i, F_j) = \frac{| F_i \cap F_j |}{| F_i \cup F_j |}
\end{equation}

The classification performance is estimated with a cross validation of 10 folds with multiple classifiers, trained and tested on the subset of features. 4 different classifiers are used: Lasso logistic regression, linear SVM, k-nearest neighbor, Random Forest.

\subsection{Results}
- Do SVM\_RFE with exponential rising
- SU achieved high values in the paper, in our results SU weighted a lot of features the 
same; therefore for robustness same weights have to be shuffled to not accidental increase
robustness by order of features
- The noise features of the NIPS 2003 challenge data sets are highly correlated with the
real feature; therefore no improvement has been made with feature selection in terms of 
accuracy
% Acknowledgements should go at the end, before appendices and references

%\acks{I want to thank my mum}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
For the feature selection methods SU and RELIEFF we used the scikit-feature library from \cite{Li-etal16}.
For SVM, RFE, Lasso logistic regression we used the scikit-sklearn library from \cite{scikit-learn}
\label{app:some appendix}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

\vskip 0.2in
\bibliography{references}

\end{document}
