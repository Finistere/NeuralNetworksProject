\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

%\ShortHeadings{Ensemble Methods for Robust Feature Selection}{Meil\u{a} and Jordan}
\firstpageno{1}

\begin{document}

\title{Ensemble Methods for Robust Feature Selection}

\author{\name Marina Meil\u{a} \email mmp@stat.washington.edu \\
       \addr Department of Statistics\\
       University of Washington\\
       Seattle, WA 98195-4322, USA
       \AND
       \name Michael I.\ Jordan \email jordan@cs.berkeley.edu \\
       \addr Division of Computer Science and Department of Statistics\\
       University of California\\
       Berkeley, CA 94720-1776, USA}

\editor{Leslie Pack Kaelbling}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Abstract
\end{abstract}

\begin{keywords}
  Ensemble Methods, Robust, 
\end{keywords}

\section{Introduction}

Feature selection is a preprocessing step used in machine learning application to find
a small subset of features in order to build a more accurate, simpler and faster model.
Moreover it allows domain experts to gain a better understanding of the data, by focusing
 on the selected subset.However in domains such as biomedical applications, 
where subsequent analysis are costly, not only the model performance but also the robustness
 of the feature selection method is important. Domain experts would prefer a more stable
 algorithm to have more confidence in the selected features. One approach for more robust
 results are ensemble methods.

In ensemble learning multiple algorithms are combined to obtain better performance or stability.
We build on the work of \cite{saeys2008} who applied ensemble learning by using
the same feature selector on different subsets. In this work we investigate the effect of ensemble methods
further by using multiple feature selection methods. In order to find a good trade-off between robustness
and model performance, multiple feature selection methods were combined in different ways.

\section{Method}
A entropy based measurement. It measures the influence of the class on a feature entropy-wise and normalizes it

We first calculate the weights if avaible by the features selector otherwise the ranks.
For feature selector, which do not omit normalized weights between a0 and 1, we normalize
the weights to the range $[0,1]$. We also normalize the ranks of the features selectors returning
ranks to the range $[0,1]$. Now that we combine the weights after one of the following rules: \ldots

\section{Weaknesses}
- Do SVM\_RFE with exponential rising
- SU achieved high values in the paper, in our results SU weighted a lot of features the 
same; therefore for robustness same weights have to be shuffled to not accidental increase
robustness by order of features
% Acknowledgements should go at the end, before appendices and references

%\acks{I want to thank my mum}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
%\section*{Appendix A.}
%\label{app:some appendix}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

\vskip 0.2in
\bibliography{references}

\end{document}
